{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피클 파일\n",
    "path = \".\\\\data\\\\txt_pkl_v3\\\\txt_pkl_v3\\\\\"\n",
    "data_list = os.listdir(path)\n",
    "len(data_list)\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "\n",
    "for data_name in data_list:\n",
    "    with open(path + data_name, 'rb') as f:\n",
    "        data_dict[data_name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>텍스트</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>주가번호</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>383646</th>\n",
       "      <td>기업은행: 2017년(E) 지배주주순이익 +30.2%yoy인 1.51조원을 예상한...</td>\n",
       "      <td>0.007559</td>\n",
       "      <td>0.023758</td>\n",
       "      <td>0.014039</td>\n",
       "      <td>0.019438</td>\n",
       "      <td>0.012959</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>0.016199</td>\n",
       "      <td>0.014039</td>\n",
       "      <td>0.015119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026998</td>\n",
       "      <td>-0.032397</td>\n",
       "      <td>-0.029158</td>\n",
       "      <td>-0.023758</td>\n",
       "      <td>-0.028078</td>\n",
       "      <td>-0.057235</td>\n",
       "      <td>-0.074514</td>\n",
       "      <td>-0.064795</td>\n",
       "      <td>-0.066955</td>\n",
       "      <td>-0.068035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383617</th>\n",
       "      <td>막연한 우려 대비 견조한 펀더멘탈3분기 지배순익 2,152억원(YoY +115.1%...</td>\n",
       "      <td>-0.028422</td>\n",
       "      <td>-0.012923</td>\n",
       "      <td>-0.010342</td>\n",
       "      <td>0.036166</td>\n",
       "      <td>0.038748</td>\n",
       "      <td>-0.005179</td>\n",
       "      <td>0.01032</td>\n",
       "      <td>-0.007766</td>\n",
       "      <td>-0.005185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304807</td>\n",
       "      <td>-0.312557</td>\n",
       "      <td>-0.315138</td>\n",
       "      <td>-0.30222</td>\n",
       "      <td>-0.299639</td>\n",
       "      <td>-0.30222</td>\n",
       "      <td>-0.30222</td>\n",
       "      <td>-0.286721</td>\n",
       "      <td>-0.278972</td>\n",
       "      <td>-0.286721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383236</th>\n",
       "      <td>실적 Review2017.11.15 What’s new : 제일제당의 삼성생명 지분...</td>\n",
       "      <td>-0.028422</td>\n",
       "      <td>-0.012923</td>\n",
       "      <td>-0.010342</td>\n",
       "      <td>0.036166</td>\n",
       "      <td>0.038748</td>\n",
       "      <td>-0.005179</td>\n",
       "      <td>0.01032</td>\n",
       "      <td>-0.007766</td>\n",
       "      <td>-0.005185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304807</td>\n",
       "      <td>-0.312557</td>\n",
       "      <td>-0.315138</td>\n",
       "      <td>-0.30222</td>\n",
       "      <td>-0.299639</td>\n",
       "      <td>-0.30222</td>\n",
       "      <td>-0.30222</td>\n",
       "      <td>-0.286721</td>\n",
       "      <td>-0.278972</td>\n",
       "      <td>-0.286721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383453</th>\n",
       "      <td>이제는 내년 소비재 주가상승 기대감으로 매수 Nov 201715 ▶ Highligh...</td>\n",
       "      <td>-0.028422</td>\n",
       "      <td>-0.012923</td>\n",
       "      <td>-0.010342</td>\n",
       "      <td>0.036166</td>\n",
       "      <td>0.038748</td>\n",
       "      <td>-0.005179</td>\n",
       "      <td>0.01032</td>\n",
       "      <td>-0.007766</td>\n",
       "      <td>-0.005185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304807</td>\n",
       "      <td>-0.312557</td>\n",
       "      <td>-0.315138</td>\n",
       "      <td>-0.30222</td>\n",
       "      <td>-0.299639</td>\n",
       "      <td>-0.30222</td>\n",
       "      <td>-0.30222</td>\n",
       "      <td>-0.286721</td>\n",
       "      <td>-0.278972</td>\n",
       "      <td>-0.286721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383647</th>\n",
       "      <td>기업은행: 2017년(E) 지배주주순이익 +30.2%yoy인 1.51조원을 예상한...</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.029692</td>\n",
       "      <td>0.045599</td>\n",
       "      <td>0.057264</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.051962</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.07211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04772</td>\n",
       "      <td>0.034995</td>\n",
       "      <td>0.02439</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.060445</td>\n",
       "      <td>0.098621</td>\n",
       "      <td>0.082715</td>\n",
       "      <td>0.093319</td>\n",
       "      <td>0.098621</td>\n",
       "      <td>0.093319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383209</th>\n",
       "      <td>Results Comment 104,00071,400 투자의견 BUY, 목표주가 1...</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.092033</td>\n",
       "      <td>0.092033</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.111264</td>\n",
       "      <td>0.135989</td>\n",
       "      <td>...</td>\n",
       "      <td>1.368132</td>\n",
       "      <td>1.364698</td>\n",
       "      <td>1.275412</td>\n",
       "      <td>1.196429</td>\n",
       "      <td>1.241071</td>\n",
       "      <td>1.107143</td>\n",
       "      <td>1.213599</td>\n",
       "      <td>1.131181</td>\n",
       "      <td>1.103709</td>\n",
       "      <td>1.110577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383195</th>\n",
       "      <td>3분기 실적 리뷰: 시장 기대치 상회 3분기 연결 매출 5,693원(YoY +224...</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.092033</td>\n",
       "      <td>0.092033</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.111264</td>\n",
       "      <td>0.135989</td>\n",
       "      <td>...</td>\n",
       "      <td>1.368132</td>\n",
       "      <td>1.364698</td>\n",
       "      <td>1.275412</td>\n",
       "      <td>1.196429</td>\n",
       "      <td>1.241071</td>\n",
       "      <td>1.107143</td>\n",
       "      <td>1.213599</td>\n",
       "      <td>1.131181</td>\n",
       "      <td>1.103709</td>\n",
       "      <td>1.110577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383305</th>\n",
       "      <td>- 국내 턴어라운드는 시장의 기대보다 가파른 상황이며, Acushnet와 USA도 ...</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.092033</td>\n",
       "      <td>0.092033</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.111264</td>\n",
       "      <td>0.135989</td>\n",
       "      <td>...</td>\n",
       "      <td>1.368132</td>\n",
       "      <td>1.364698</td>\n",
       "      <td>1.275412</td>\n",
       "      <td>1.196429</td>\n",
       "      <td>1.241071</td>\n",
       "      <td>1.107143</td>\n",
       "      <td>1.213599</td>\n",
       "      <td>1.131181</td>\n",
       "      <td>1.103709</td>\n",
       "      <td>1.110577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383293</th>\n",
       "      <td>Result Comment 3분기 연결 영업이익은 471.3% YoY 증가한 356...</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.092033</td>\n",
       "      <td>0.092033</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.111264</td>\n",
       "      <td>0.135989</td>\n",
       "      <td>...</td>\n",
       "      <td>1.368132</td>\n",
       "      <td>1.364698</td>\n",
       "      <td>1.275412</td>\n",
       "      <td>1.196429</td>\n",
       "      <td>1.241071</td>\n",
       "      <td>1.107143</td>\n",
       "      <td>1.213599</td>\n",
       "      <td>1.131181</td>\n",
       "      <td>1.103709</td>\n",
       "      <td>1.110577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383594</th>\n",
       "      <td>미국과 유럽 출시 스케줄을 주목할 필요 2018년도 매출액은 1,958억원(+10....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008893</td>\n",
       "      <td>-0.011526</td>\n",
       "      <td>-0.02952</td>\n",
       "      <td>-0.090173</td>\n",
       "      <td>-0.105333</td>\n",
       "      <td>-0.070963</td>\n",
       "      <td>-0.078036</td>\n",
       "      <td>-0.063682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03539</td>\n",
       "      <td>-0.039836</td>\n",
       "      <td>-0.03357</td>\n",
       "      <td>-0.034981</td>\n",
       "      <td>-0.002229</td>\n",
       "      <td>-0.015771</td>\n",
       "      <td>-0.012136</td>\n",
       "      <td>-0.037002</td>\n",
       "      <td>-0.042659</td>\n",
       "      <td>-0.037406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      텍스트         0         1  \\\n",
       "주가번호                                                                            \n",
       "383646   기업은행: 2017년(E) 지배주주순이익 +30.2%yoy인 1.51조원을 예상한...  0.007559  0.023758   \n",
       "383617  막연한 우려 대비 견조한 펀더멘탈3분기 지배순익 2,152억원(YoY +115.1%... -0.028422 -0.012923   \n",
       "383236  실적 Review2017.11.15 What’s new : 제일제당의 삼성생명 지분... -0.028422 -0.012923   \n",
       "383453  이제는 내년 소비재 주가상승 기대감으로 매수 Nov 201715 ▶ Highligh... -0.028422 -0.012923   \n",
       "383647   기업은행: 2017년(E) 지배주주순이익 +30.2%yoy인 1.51조원을 예상한...  0.011665  0.029692   \n",
       "...                                                   ...       ...       ...   \n",
       "383209  Results Comment 104,00071,400 투자의견 BUY, 목표주가 1...  0.019231  0.038462   \n",
       "383195  3분기 실적 리뷰: 시장 기대치 상회 3분기 연결 매출 5,693원(YoY +224...  0.019231  0.038462   \n",
       "383305  - 국내 턴어라운드는 시장의 기대보다 가파른 상황이며, Acushnet와 USA도 ...  0.019231  0.038462   \n",
       "383293  Result Comment 3분기 연결 영업이익은 471.3% YoY 증가한 356...  0.019231  0.038462   \n",
       "383594  미국과 유럽 출시 스케줄을 주목할 필요 2018년도 매출액은 1,958억원(+10....       0.0  0.008893   \n",
       "\n",
       "               2         3         4         5         6         7         8  \\\n",
       "주가번호                                                                           \n",
       "383646  0.014039  0.019438  0.012959  0.011879  0.016199  0.014039  0.015119   \n",
       "383617 -0.010342  0.036166  0.038748 -0.005179   0.01032 -0.007766 -0.005185   \n",
       "383236 -0.010342  0.036166  0.038748 -0.005179   0.01032 -0.007766 -0.005185   \n",
       "383453 -0.010342  0.036166  0.038748 -0.005179   0.01032 -0.007766 -0.005185   \n",
       "383647  0.045599  0.057264  0.043478  0.051962  0.054083  0.043478   0.07211   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "383209  0.065934  0.092033  0.092033  0.090659  0.090659  0.111264  0.135989   \n",
       "383195  0.065934  0.092033  0.092033  0.090659  0.090659  0.111264  0.135989   \n",
       "383305  0.065934  0.092033  0.092033  0.090659  0.090659  0.111264  0.135989   \n",
       "383293  0.065934  0.092033  0.092033  0.090659  0.090659  0.111264  0.135989   \n",
       "383594 -0.011526  -0.02952 -0.090173 -0.105333 -0.070963 -0.078036 -0.063682   \n",
       "\n",
       "        ...       170       171       172       173       174       175  \\\n",
       "주가번호    ...                                                               \n",
       "383646  ... -0.026998 -0.032397 -0.029158 -0.023758 -0.028078 -0.057235   \n",
       "383617  ... -0.304807 -0.312557 -0.315138  -0.30222 -0.299639  -0.30222   \n",
       "383236  ... -0.304807 -0.312557 -0.315138  -0.30222 -0.299639  -0.30222   \n",
       "383453  ... -0.304807 -0.312557 -0.315138  -0.30222 -0.299639  -0.30222   \n",
       "383647  ...   0.04772  0.034995   0.02439  0.003181  0.060445  0.098621   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "383209  ...  1.368132  1.364698  1.275412  1.196429  1.241071  1.107143   \n",
       "383195  ...  1.368132  1.364698  1.275412  1.196429  1.241071  1.107143   \n",
       "383305  ...  1.368132  1.364698  1.275412  1.196429  1.241071  1.107143   \n",
       "383293  ...  1.368132  1.364698  1.275412  1.196429  1.241071  1.107143   \n",
       "383594  ...  -0.03539 -0.039836  -0.03357 -0.034981 -0.002229 -0.015771   \n",
       "\n",
       "             176       177       178       179  \n",
       "주가번호                                            \n",
       "383646 -0.074514 -0.064795 -0.066955 -0.068035  \n",
       "383617  -0.30222 -0.286721 -0.278972 -0.286721  \n",
       "383236  -0.30222 -0.286721 -0.278972 -0.286721  \n",
       "383453  -0.30222 -0.286721 -0.278972 -0.286721  \n",
       "383647  0.082715  0.093319  0.098621  0.093319  \n",
       "...          ...       ...       ...       ...  \n",
       "383209  1.213599  1.131181  1.103709  1.110577  \n",
       "383195  1.213599  1.131181  1.103709  1.110577  \n",
       "383305  1.213599  1.131181  1.103709  1.110577  \n",
       "383293  1.213599  1.131181  1.103709  1.110577  \n",
       "383594 -0.012136 -0.037002 -0.042659 -0.037406  \n",
       "\n",
       "[261 rows x 181 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['20171115.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 세팅y\n",
    "# batch size : 64\n",
    "# learning_late : 3e-5\n",
    "# data_dir :\n",
    "# sts_eval_dir :\n",
    "# output_dir :\n",
    "# epoch : 1 - unsupervised learning\n",
    "# temperature : 0.05\n",
    "# 250번마다 평가\n",
    "# eval_logging_interval : 250\n",
    "# seed : 42\n",
    "# device = \"cuda:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    model_name = \"snunlp/KR-FinBert\"\n",
    "    dataset_dir = \".\\\\data\\\\txt_files_f1\\\\txt_files_f1\\\\\"\n",
    "    learning_rate = 3e-5\n",
    "    batch_size = 8\n",
    "    sts_eval_dir = \".\\\\sts\"\n",
    "    output_dir = \".\\\\output\"\n",
    "    epochs = 1\n",
    "    temperature = 0.05\n",
    "    eval_logging_interval = 250\n",
    "    seed = 42\n",
    "    num_warmup_steps = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일\n",
    "path_csv = Args.dataset_dir\n",
    "data_list = os.listdir(path_csv)\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "for data_name in data_list:\n",
    "    data_df = pd.concat([data_df, pd.read_csv(path_csv+data_name)], axis=0)\n",
    "\n",
    "data_df.rename(columns={\"Unnamed: 0\" : '주가번호'}, inplace=True)\n",
    "data_df.reset_index(drop=True, inplace=True)\n",
    "data_df.rename(columns={'0':'report'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>기업은행: 2017년(E) 지배주주순이익 +30.2%yoy인 1.51조원을 예상한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>막연한 우려 대비 견조한 펀더멘탈3분기 지배순익 2,152억원(YoY +115.1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>실적 Review2017.11.15 What’s new : 제일제당의 삼성생명 지분...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이제는 내년 소비재 주가상승 기대감으로 매수 Nov 201715 ▶ Highligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>기업은행: 2017년(E) 지배주주순이익 +30.2%yoy인 1.51조원을 예상한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117843</th>\n",
       "      <td>2022년 5월 19일 우호적인 업황 속 대표 수혜주목표주가 150,000원, 투자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117844</th>\n",
       "      <td>2022년 5월 19일 견조한 실적 유지. 전기차 모멘텀 기대하반기에도 견조한 실적...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117845</th>\n",
       "      <td>BUY(신규)75,000원(신규) (057050)실적 모멘텀 둔화 현대홈쇼핑에 대해...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117846</th>\n",
       "      <td>BUY(신규)90,000원(신규)75,500원 (008770)이륙 준비 완료호텔신라...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117847</th>\n",
       "      <td>Buy (유지)목표주가(하향): 23,000원 환인제약의 1분기는 약가인하에도 준수...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117848 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   report\n",
       "0        기업은행: 2017년(E) 지배주주순이익 +30.2%yoy인 1.51조원을 예상한...\n",
       "1       막연한 우려 대비 견조한 펀더멘탈3분기 지배순익 2,152억원(YoY +115.1%...\n",
       "2       실적 Review2017.11.15 What’s new : 제일제당의 삼성생명 지분...\n",
       "3       이제는 내년 소비재 주가상승 기대감으로 매수 Nov 201715 ▶ Highligh...\n",
       "4        기업은행: 2017년(E) 지배주주순이익 +30.2%yoy인 1.51조원을 예상한...\n",
       "...                                                   ...\n",
       "117843  2022년 5월 19일 우호적인 업황 속 대표 수혜주목표주가 150,000원, 투자...\n",
       "117844  2022년 5월 19일 견조한 실적 유지. 전기차 모멘텀 기대하반기에도 견조한 실적...\n",
       "117845  BUY(신규)75,000원(신규) (057050)실적 모멘텀 둔화 현대홈쇼핑에 대해...\n",
       "117846  BUY(신규)90,000원(신규)75,500원 (008770)이륙 준비 완료호텔신라...\n",
       "117847  Buy (유지)목표주가(하향): 23,000원 환인제약의 1분기는 약가인하에도 준수...\n",
       "\n",
       "[117848 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Anaconda3\\envs\\base_torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "report = pd.DataFrame(data_df['report'])\n",
    "custom_dataset = Dataset.from_pandas(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at snunlp/KR-FinBert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, BatchEncoding\n",
    "tokenizer = AutoTokenizer.from_pretrained(Args.model_name)\n",
    "pretrain_model = AutoModel.from_pretrained(Args.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 117848/117848 [00:52<00:00, 2258.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "custom_dataset = custom_dataset.map(lambda x: tokenizer(x['report'], max_length = 512, truncation=True, padding=\"max_length\"), batched=True, remove_columns=['report']).with_format(\"torch\", columns = ['input_ids', 'token_type_ids', 'attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "# train_test_split하기\n",
    "\n",
    "train_size = 0.8\n",
    "validate_size = 0.1\n",
    "test_size = 1- train_size-validate_size\n",
    "\n",
    "\n",
    "train_dataset, validate_dataset, test_dataset = random_split(custom_dataset, [train_size, validate_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, Args.batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(validate_dataset, Args.batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, Args.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 11785\n",
      "Validation Data Size : 1474\n",
      "Testing Data Size : 1473\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Size : {len(train_dataloader)}\")\n",
    "print(f\"Validation Data Size : {len(valid_dataloader)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 불러오기\n",
    "2. pretrain 모델 불러오기\n",
    "3. 데이터의 embedding 만들기\n",
    "4. unsupervised 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_config = AutoConfig.from_pretrained(\"snunlp/KR-FinBert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"snunlp/KR-FinBert\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 20000\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised learning에서는 학습과정에 MLP를 넣어야함\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class SimCSEmodel(nn.Module):\n",
    "    def __init__(self, pretrain_model):\n",
    "        super().__init__()\n",
    "        # pretrain_model 불러오기\n",
    "        self.model = pretrain_model\n",
    "        # pretrain_model에서 hidden state를 가져와서 MLP 모델 통과하기\n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        # input : hidden size, output : hidden size\n",
    "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "            \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        ## output에서 pooling 된 laste hidden states 얻기위함\n",
    "        outputs = BaseModelOutputWithPoolingAndCrossAttentions(self.model(input_ids = input_ids, attention_mask = attention_mask, token_type_ids=token_type_ids))\n",
    "        # [CLS] 토큰을 embedding 한 값 얻기\n",
    "        # max pooling or mean pooling 사용가능\n",
    "        embedding_value = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        # 학습시 MLP layer 사용하기에 아래와 같이 코드를 작성\n",
    "        if self.train():\n",
    "            embedding_value = self.dense(embedding_value)\n",
    "            embedding_value = self.activation(embedding_value)\n",
    "        \n",
    "        return embedding_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimCSEmodel(pretrain_model).to(Args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 코드 구현\n",
    "# batch size = 64, learning rate 3e-5\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=Args.learning_rate)\n",
    "\n",
    "lr_scheduler =transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=Args.num_warmup_steps,\n",
    "    # len(train_dataloader) is the number of steps in one epoch\n",
    "    num_training_steps=len(train_dataloader) * Args.epochs,\n",
    ")\n",
    "\n",
    "def train(model, optimizer, train_data, val_data):\n",
    "    for epoch in range(Args.epochs):\n",
    "        model.train()\n",
    "        for step, batch in tqdm(enumerate(train_data)):\n",
    "            batch_data = BatchEncoding(batch).to(Args.device)\n",
    "\n",
    "    \n",
    "            # forward input을 2번 통과하기 위함\n",
    "            # 다른 dropout mask가 자동적으로 적용된다.\n",
    "\n",
    "            emb1 = model.forward(**batch_data)\n",
    "            emb2 = model.forward(**batch_data)\n",
    "\n",
    "            # emb1, emb2 pair간의 유사도를 비슷하게 확인하기 위해 코사인 유사도 계산\n",
    "\n",
    "            emb1 = emb1.unsqueeze(1)\n",
    "            emb2 = emb2.unsqueeze(0)\n",
    "\n",
    "            sim_matrix = F.cosine_similarity(emb1, emb2, dim=-1)\n",
    "            # temperature을 나누어줌\n",
    "            sim_matrix = sim_matrix / Args.temperature\n",
    "\n",
    "            # labels : 대각원소의 인덱스를 알려주는 역할(positive examples끼리 묶는용도)\n",
    "            labels = torch.arange(Args.batch_size).long().to(Args.device)\n",
    "\n",
    "            # cross entropy 사용 : softmax 효과를 주고 대각행렬의 유사도를 최대화할 수 있기때문\n",
    "\n",
    "            loss = F.cross_entropy(sim_matrix, labels)    \n",
    "\n",
    "            # 기울기 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 8.00 GiB total capacity; 7.28 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(model, optimizer, train_dataloader, valid_dataloader)\n",
      "Cell \u001b[1;32mIn[23], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_data, val_data)\u001b[0m\n\u001b[0;32m     20\u001b[0m batch_data \u001b[39m=\u001b[39m BatchEncoding(batch)\u001b[39m.\u001b[39mto(Args\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     23\u001b[0m \u001b[39m# forward input을 2번 통과하기 위함\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# 다른 dropout mask가 자동적으로 적용된다.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m emb1 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch_data)\n\u001b[0;32m     27\u001b[0m emb2 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch_data)\n\u001b[0;32m     29\u001b[0m \u001b[39m# emb1, emb2 pair간의 유사도를 비슷하게 확인하기 위해 코사인 유사도 계산\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m, in \u001b[0;36mSimCSEmodel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask, token_type_ids):\n\u001b[0;32m     20\u001b[0m     \u001b[39m## output에서 pooling 된 laste hidden states 얻기위함\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     outputs \u001b[39m=\u001b[39m BaseModelOutputWithPoolingAndCrossAttentions(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids \u001b[39m=\u001b[39;49m input_ids, attention_mask \u001b[39m=\u001b[39;49m attention_mask, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids))\n\u001b[0;32m     22\u001b[0m     \u001b[39m# [CLS] 토큰을 embedding 한 값 얻기\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[39m# max pooling or mean pooling 사용가능\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     embedding_value \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\Anaconda3\\envs\\base_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\Anaconda3\\envs\\base_torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1015\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1008\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1018\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1019\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\Anaconda3\\envs\\base_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\Anaconda3\\envs\\base_torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:232\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    229\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    231\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[0;32m    233\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    235\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\user\\Anaconda3\\envs\\base_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\Anaconda3\\envs\\base_torch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\user\\Anaconda3\\envs\\base_torch\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 8.00 GiB total capacity; 7.28 GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가지표 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 3            |        cudaMalloc retries: 3         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   7328 MiB |   7437 MiB |  27411 MiB |  20082 MiB |\\n|       from large pool |   7327 MiB |   7435 MiB |  27407 MiB |  20080 MiB |\\n|       from small pool |      1 MiB |      2 MiB |      3 MiB |      2 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   7328 MiB |   7437 MiB |  27411 MiB |  20082 MiB |\\n|       from large pool |   7327 MiB |   7435 MiB |  27407 MiB |  20080 MiB |\\n|       from small pool |      1 MiB |      2 MiB |      3 MiB |      2 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   7328 MiB |   7436 MiB |  27409 MiB |  20081 MiB |\\n|       from large pool |   7326 MiB |   7434 MiB |  27405 MiB |  20079 MiB |\\n|       from small pool |      1 MiB |      2 MiB |      3 MiB |      2 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   7474 MiB |   7476 MiB |   7476 MiB |   2048 KiB |\\n|       from large pool |   7472 MiB |   7472 MiB |   7472 MiB |      0 KiB |\\n|       from small pool |      2 MiB |      4 MiB |      4 MiB |   2048 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 148586 KiB | 478050 KiB |  12065 MiB |  11920 MiB |\\n|       from large pool | 148256 KiB | 476960 KiB |  12059 MiB |  11914 MiB |\\n|       from small pool |    330 KiB |   3442 KiB |      6 MiB |      5 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     530    |     531    |    1197    |     667    |\\n|       from large pool |     330    |     331    |     816    |     486    |\\n|       from small pool |     200    |     254    |     381    |     181    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     530    |     531    |    1197    |     667    |\\n|       from large pool |     330    |     331    |     816    |     486    |\\n|       from small pool |     200    |     254    |     381    |     181    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     131    |     132    |     132    |       1    |\\n|       from large pool |     130    |     130    |     130    |       0    |\\n|       from small pool |       1    |       2    |       2    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      38    |      44    |     361    |     323    |\\n|       from large pool |      36    |      42    |     331    |     295    |\\n|       from small pool |       2    |       5    |      30    |      28    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
